{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0df5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Header Level                    Header Text\n",
      "0            h1                      Main Page\n",
      "1            h1           Welcome to Wikipedia\n",
      "2            h2  From today's featured article\n",
      "3            h2               Did you knowÂ ...\n",
      "4            h2                    In the news\n",
      "5            h2                    On this day\n",
      "6            h2     From today's featured list\n",
      "7            h2       Today's featured picture\n",
      "8            h2       Other areas of Wikipedia\n",
      "9            h2    Wikipedia's sister projects\n",
      "10           h2            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    headers_data = {\n",
    "        'Header Level': [],\n",
    "        'Header Text': []\n",
    "    }\n",
    "\n",
    "    for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        header_level = header.name\n",
    "        header_text = header.text.strip()\n",
    "\n",
    "        headers_data['Header Level'].append(header_level)\n",
    "        headers_data['Header Text'].append(header_text)\n",
    "\n",
    "    return headers_data\n",
    "\n",
    "wikipedia_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "data = scrape_wikipedia_headers(wikipedia_url)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7091e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_former_presidents(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    president_data = {\n",
    "        'Name': [],\n",
    "        'Term of Office': []\n",
    "    }\n",
    "\n",
    "    for president in soup.find_all('table', {'class': 'views-table'}):\n",
    "        name = president.find('td', {'class': 'views-field-field-president'}).text.strip()\n",
    "        term_of_office = president.find('td', {'class': 'views-field-field-tenure'}).text.strip()\n",
    "\n",
    "        president_data['Name'].append(name)\n",
    "        president_data['Term of Office'].append(term_of_office)\n",
    "\n",
    "    return president_data\n",
    "\n",
    "president_url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "data = scrape_former_presidents(president_url)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565eaf3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 80\u001b[0m\n\u001b[0;32m     76\u001b[0m         bowlers_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(rating)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(bowlers_data)\n\u001b[1;32m---> 80\u001b[0m odi_teams_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_top_odi_teams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m odi_batsmen_df \u001b[38;5;241m=\u001b[39m scrape_top_odi_batsmen()\n\u001b[0;32m     82\u001b[0m odi_bowlers_df \u001b[38;5;241m=\u001b[39m scrape_top_odi_bowlers()\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mscrape_top_odi_teams\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m matches \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     19\u001b[0m points \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 20\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(team)\n\u001b[0;32m     23\u001b[0m team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(matches)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def scrape_top_odi_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    team_data = {\n",
    "        'Team': [],\n",
    "        'Matches': [],\n",
    "        'Points': [],\n",
    "        'Rating': []\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        matches = columns[3].text.strip()\n",
    "        points = columns[4].text.strip()\n",
    "        rating = columns[5].text.strip()\n",
    "\n",
    "        team_data['Team'].append(team)\n",
    "        team_data['Matches'].append(matches)\n",
    "        team_data['Points'].append(points)\n",
    "        team_data['Rating'].append(rating)\n",
    "\n",
    "    return pd.DataFrame(team_data)\n",
    "\n",
    "def scrape_top_odi_batsmen():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batsmen_data = {\n",
    "        'Batsman': [],\n",
    "        'Team': [],\n",
    "        'Rating': []\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        batsman = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        batsmen_data['Batsman'].append(batsman)\n",
    "        batsmen_data['Team'].append(team)\n",
    "        batsmen_data['Rating'].append(rating)\n",
    "\n",
    "    return pd.DataFrame(batsmen_data)\n",
    "\n",
    "\n",
    "def scrape_top_odi_bowlers():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    bowlers_data = {\n",
    "        'Bowler': [],\n",
    "        'Team': [],\n",
    "        'Rating': []\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        bowler = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        bowlers_data['Bowler'].append(bowler)\n",
    "        bowlers_data['Team'].append(team)\n",
    "        bowlers_data['Rating'].append(rating)\n",
    "\n",
    "    return pd.DataFrame(bowlers_data)\n",
    "\n",
    "odi_teams_df = scrape_top_odi_teams()\n",
    "odi_batsmen_df = scrape_top_odi_batsmen()\n",
    "odi_bowlers_df = scrape_top_odi_bowlers()\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(odi_teams_df)\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(odi_batsmen_df)\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(odi_bowlers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a431a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     76\u001b[0m         allrounders_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(rating)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(allrounders_data)\n\u001b[1;32m---> 81\u001b[0m odi_womens_teams_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_top_odi_womens_teams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m odi_womens_batting_df \u001b[38;5;241m=\u001b[39m scrape_top_odi_womens_batting_players()\n\u001b[0;32m     83\u001b[0m odi_womens_allrounders_df \u001b[38;5;241m=\u001b[39m scrape_top_odi_womens_allrounders()\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mscrape_top_odi_womens_teams\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m matches \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     19\u001b[0m points \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 20\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(team)\n\u001b[0;32m     23\u001b[0m team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(matches)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def scrape_top_odi_womens_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    team_data = {\n",
    "        'Team': [],\n",
    "        'Matches': [],\n",
    "        'Points': [],\n",
    "        'Rating': []\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        matches = columns[3].text.strip()\n",
    "        points = columns[4].text.strip()\n",
    "        rating = columns[5].text.strip()\n",
    "\n",
    "        team_data['Team'].append(team)\n",
    "        team_data['Matches'].append(matches)\n",
    "        team_data['Points'].append(points)\n",
    "        team_data['Rating'].append(rating)\n",
    "\n",
    "    return pd.DataFrame(team_data)\n",
    "\n",
    "\n",
    "def scrape_top_odi_womens_batting_players():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batting_players_data = {\n",
    "        'Batsman': [],\n",
    "        'Team': [],\n",
    "        'Rating': []\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        batsman = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        batting_players_data['Batsman'].append(batsman)\n",
    "        batting_players_data['Team'].append(team)\n",
    "        batting_players_data['Rating'].append(rating)\n",
    "\n",
    "    return pd.DataFrame(batting_players_data)\n",
    "\n",
    "def scrape_top_odi_womens_allrounders():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    allrounders_data = {\n",
    "        'All-Rounder': [],\n",
    "        'Team': [],\n",
    "        'Rating': []\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        all_rounder = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        allrounders_data['All-Rounder'].append(all_rounder)\n",
    "        allrounders_data['Team'].append(team)\n",
    "        allrounders_data['Rating'].append(rating)\n",
    "\n",
    "    return pd.DataFrame(allrounders_data)\n",
    "\n",
    "\n",
    "odi_womens_teams_df = scrape_top_odi_womens_teams()\n",
    "odi_womens_batting_df = scrape_top_odi_womens_batting_players()\n",
    "odi_womens_allrounders_df = scrape_top_odi_womens_allrounders()\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI Women's Teams:\")\n",
    "print(odi_womens_teams_df)\n",
    "print(\"\\nTop 10 ODI Women's Batting Players:\")\n",
    "print(odi_womens_batting_df)\n",
    "print(\"\\nTop 10 ODI Women's All-Rounders:\")\n",
    "print(odi_womens_allrounders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beba104a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m news_data\n\u001b[0;32m     24\u001b[0m cnbc_world_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 27\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_cnbc_world_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnbc_world_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mscrape_cnbc_world_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m news_items \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-titleContainer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m news_items:\n\u001b[1;32m---> 14\u001b[0m     headline \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     15\u001b[0m     time \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     news_link \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "def scrape_cnbc_world_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    news_data = {\n",
    "        'Headline': [],\n",
    "        'Time': [],\n",
    "        'News Link': []\n",
    "    }\n",
    "\n",
    "    news_items = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "    for item in news_items:\n",
    "        headline = item.find('h3').text.strip()\n",
    "        time = item.find('time').text.strip()\n",
    "        news_link = item.find('a')['href']\n",
    "\n",
    "        news_data['Headline'].append(headline)\n",
    "        news_data['Time'].append(time)\n",
    "        news_data['News Link'].append(news_link)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "cnbc_world_url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "\n",
    "data = scrape_cnbc_world_news(cnbc_world_url)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7966f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
